ğŸ“˜ PostureGuard â€“ README.md

AI-powered posture detection with periodic camera sampling, ML training, and voice alerts
Frontend: React (with MediaPipe), Backend: Django (no DB yet), Client-side ML

## ğŸ“ Training Your Personal Model

**NEW**: See **[TRAINING_GUIDE.md](./TRAINING_GUIDE.md)** for a complete step-by-step guide on:
- Collecting 20+ samples for "good" and "slouch" postures
- Training your seat-specific model
- Testing and fine-tuning your model
- Troubleshooting common issues

Quick start: Enable camera â†’ Record Good (20x) â†’ Record Slouch (20x) â†’ Train Model

ğŸ§  Overview

PostureGuard is a smart posture monitoring system built for students and professionals who spend long hours on laptops.
Instead of keeping the camera ON continuously (which drains battery & risks privacy), our approach uses Periodic Sampling:

Camera turns on every 1 minute

Stays active for 5â€“10 seconds

Detects posture using MediaPipe BlazePose

Uses ML to classify posture as Good / Slouch

Plays voice alerts when slouch persists

Completely local â€” no video frames leave the device

This README explains the architecture, training approach, slouch detection model, integration steps, and how to wire the new ML modules into your already-built frontend.

ğŸ¯ Features
âœ” Periodic camera activation

Battery-friendly: camera is ON only 10â€“15% of the time.

âœ” MediaPipe BlazePose keypoints

Accurate face, shoulder, hip, and spine keypoints.

âœ” Custom ML model (client-side, TF.js)

User-specific model trained inside the browser.

âœ” Real-time slouch detection

Using angles + ML classification.

âœ” Voice Alerts

Uses Web Speech API:

â€œPlease sit upright and straighten your back.â€

âœ” Privacy-first

No video storage. No frames sent to backend. Only angles & labels stored.

âœ” Non-invasive integration

ML modules do not modify your existing frontend code.
They integrate via:

import, or

window.__LATEST_KEYPOINTS__ (auto-detect).

ğŸ—ï¸ Project Structure Overview
/frontend
    /src
        /components
        /ml
            poseUtils.js
            modelService.js
            trainerAPI.js
            detectorAPI.js
            storage.js
            constants.js
        App.jsx
        Landing.jsx
        CameraAccess.jsx
    package.json

/backend
    (Django without DB; in-memory store)

ğŸ”§ How Periodic Sampling Works

Your existing flow is preserved:

Every 60 seconds:
    Turn ON camera
    For 10 seconds:
       - Run BlazePose at ~5fps
       - Collect features (angles, noseâ†’shoulder distance, etc.)
       - Run ML inference
    Turn camera OFF
    If slouch persists â†’ voice alert


No changes to existing logic, the new ML just plugs in internally.

ğŸ§ğŸ» Posture Features Extracted

From BlazePose keypoints:

spine_angle

spine_angle_relative = spine_angle - baseline_angle

neck_tilt

head_forward_dist

shoulder_slope

avg_confidence

These are used by both:

Threshold-based fallback classifier, and

Trained TF.js neural network

ğŸ¤– The ML Model
Model Type

Small Sequential model:

Dense(32) â†’ Dense(16) â†’ Dense(2 softmax)

Input

5â€“6 numeric features (normalized).

Output

[P(good), P(slouch)]

Storage

Saved to IndexedDB as:

indexeddb://posture-model-v1

Training Requirements

You must collect at least 20 samples per class:

Good

Slouch

ğŸ¥ Data Collection UI (Trainer)

You will get a trainer API with functions:

startRecording("good")
startRecording("slouch")
stopRecording()
train()
loadModel()
exportDataset()
importDataset(json)


Use these from browser console or integrate UI buttons later.

ğŸ“¡ Detector API (Inference)

You will get:

initDetector({getKeypoints, onAlert, config})
startDetector()
stopDetector()
forceInferOnce()
isModelLoaded()


This does NOT modify your existing camera or periodic loop â€” it hooks into it.

ğŸ”Š Voice Alerts

Uses Web Speech API:

speechSynthesis.speak(new SpeechSynthesisUtterance("Please sit upright"));


Alerts respect:

persistence threshold (e.g., 8 seconds slouch)

cooldown (e.g., 2 minutes)

mute/snooze flags

ğŸ”Œ How to Integrate the ML Code (IMPORTANT)
Option A â€” Import (recommended)

Add this to your existing code (e.g., Landing.jsx or CameraAccess.jsx):

import { initDetector, startDetector } from "./ml/detectorAPI";

useEffect(() => {
    initDetector({
        getKeypoints: () => window.__LATEST_KEYPOINTS__, // already set in your code
        onAlert: ({label}) => console.log("ALERT:", label)
    });
    startDetector();
}, []);

Option B â€” ZERO changes (auto-detection)

If you already update:

window.__LATEST_KEYPOINTS__ = latestKeypointFrame;


Then the ML modules will auto-detect them.

ğŸ§ª Testing Guide
1. Calibrate

Sit straight â†’ press â€œCalibrateâ€.

2. Collect Data

In browser console:

trainer.startRecording("good")
trainer.startRecording("slouch")
trainer.train()

3. Load & Run Detector
detector.loadModel()
detector.startDetector()

4. Test Slouch

Lean forward â†’ hold slouch for 8 seconds â†’ voice alert triggers.

ğŸ” Privacy Notes

Your backend does NOT receive:

images

frames

videos

Data stored:

posture angles

numerical features

predicted labels

local model

calibration values

Everything stays on device.

ğŸ Backend (Django, No DB)

Stores sessions in memory

Receives only posture events & metrics

No video

No training data

No sensitive content

Backend is optional for the core MLâ€”ML is client-side.

ğŸ› ï¸ Requirements

Frontend:

npm install @tensorflow/tfjs @mediapipe/pose idb-keyval


Backend (optional):

pip install django djangorestframework pyjwt bcrypt

ğŸ“¦ Exporting / Importing Dataset

You can export dataset for sharing:

trainer.exportDataset()


And import on another machine:

trainer.importDataset(jsonData)

ğŸ¤ Example Console Flow (copy & paste)
1. Train Model
await trainer.startRecording("good", 6000)
await trainer.startRecording("slouch", 6000)
await trainer.train({epochs: 20})

2. Start Live Detection
await detector.loadModel()
detector.startDetector()

ğŸ§© Troubleshooting
ğŸ”¸ Model always predicts â€œGoodâ€

Not enough slouch samples

Poor lighting

Wrong keypoints

Need calibration

Model not loaded (using fallback)

ğŸ”¸ Voice not playing

Browser blocked audio â†’ perform one user click first

Use startDetector() after a button click

ğŸ”¸ Keypoints undefined

Check your camera component sets:

window.__LATEST_KEYPOINTS__